{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22i-1914\n",
    "#QUESTION#1\n",
    "#Talha Kayani\n",
    "import pandas as pd\n",
    "#loading data\n",
    "property_data=pd.read_csv('property.csv')\n",
    "#filtering data for just the specified city which is i am taking Islamabad\n",
    "selected_city='Islamabad'\n",
    "#creating new data frame as isl_data only having islamabad's data\n",
    "isl_data=property_data[property_data['city']==selected_city]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_13956\\1221796206.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isl_data['date_added']=pd.to_datetime(isl_data['date_added'])\n",
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_13956\\1221796206.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isl_data['agency'].fillna(isl_data['agency'].mode()[0], inplace=True)\n",
      "C:\\Users\\leads trading\\AppData\\Local\\Temp\\ipykernel_13956\\1221796206.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isl_data['agent'].fillna(isl_data['agent'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Data Pre-processing\n",
    "\n",
    "#changing datatype of date-added to date-time.\n",
    "isl_data['date_added']=pd.to_datetime(isl_data['date_added'])\n",
    "#i get to know that there are missing values in agency and agent column so i filled them up\n",
    "isl_data['agency'].fillna(isl_data['agency'].mode()[0], inplace=True)\n",
    "isl_data['agent'].fillna(isl_data['agent'].mode()[0], inplace=True)\n",
    "\n",
    "#removing outliers from the data to make a precise model\n",
    "def remove_outliers(data, columns, threshold=1.5):\n",
    "   \n",
    "    data_no_outliers = data.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "       \n",
    "        Q1 = data_no_outliers[column].quantile(0.25)\n",
    "        Q3 = data_no_outliers[column].quantile(0.75)\n",
    "        \n",
    "        \n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        \n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        # Remove outliers for the current column\n",
    "        data_no_outliers = data_no_outliers[(data_no_outliers[column] >= lower_bound) & (data_no_outliers[column] <= upper_bound)]\n",
    "    \n",
    "    return data_no_outliers\n",
    "\n",
    "# List of numeric columns in isl_data\n",
    "numeric_columns = ['price', 'baths', 'latitude', 'longitude', 'bedrooms']\n",
    "\n",
    "# Removing outliers from all numeric columns\n",
    "isldata = remove_outliers(isl_data, numeric_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Variable 1   Variable 2  Correlation\n",
      "0  property_id  property_id     1.000000\n",
      "1  location_id  location_id     1.000000\n",
      "2        price        price     1.000000\n",
      "3     latitude     latitude     1.000000\n",
      "4    longitude    longitude     1.000000\n",
      "5        baths        baths     1.000000\n",
      "6        baths     bedrooms     0.788133\n",
      "7     bedrooms        baths     0.788133\n",
      "8     bedrooms     bedrooms     1.000000\n",
      "Correlation between properties count and average price for agents: -0.023466138355901427\n",
      "Correlation between properties count and average price for agencies: -0.03776101284324156\n"
     ]
    }
   ],
   "source": [
    "#Exploratory Data Analysis (EDA)\n",
    "\n",
    "#Correlations\n",
    "numeric_columns = isldata.select_dtypes(include='number')\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "# Printing correlation values for specific pairs\n",
    "#I have choosed 0.7 to only viw notably high or low relations\n",
    "notable_correlations = correlation_matrix[(correlation_matrix > 0.7) | (correlation_matrix < -0.7)].stack().reset_index()\n",
    "notable_correlations.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "print(notable_correlations)\n",
    "\n",
    "\n",
    "#checking correlation between the number of properties listed by an agent,agency and the average property price\n",
    "properties_per_agent = isldata['agent'].value_counts()\n",
    "properties_per_agency = isldata['agency'].value_counts()\n",
    "average_price_per_agent = isldata.groupby('agent')['price'].mean()\n",
    "average_price_per_agency = isldata.groupby('agency')['price'].mean()\n",
    "\n",
    "# merging the calculated metrics into a single DataFrame\n",
    "agent_metrics = pd.DataFrame({\n",
    "    'properties_count': properties_per_agent,\n",
    "    'average_price': average_price_per_agent\n",
    "})\n",
    "\n",
    "agency_metrics = pd.DataFrame({\n",
    "    'properties_count': properties_per_agency,\n",
    "    'average_price': average_price_per_agency\n",
    "})\n",
    "\n",
    "# Correlation for agents\n",
    "agent_correlation = agent_metrics['properties_count'].corr(agent_metrics['average_price'])\n",
    "\n",
    "# Correlation for agencies\n",
    "agency_correlation = agency_metrics['properties_count'].corr(agency_metrics['average_price'])\n",
    "\n",
    "print(f\"Correlation between properties count and average price for agents: {agent_correlation}\")\n",
    "print(f\"Correlation between properties count and average price for agencies: {agency_correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "#computing new column indicating price per square meter\n",
    "kanal_to_sq_meter = 505.857\n",
    "marla_to_sq_meter = 25.2929\n",
    "\n",
    "# Function to convert 'area' to square meters\n",
    "def convert_to_sq_meter(row):\n",
    "    try:\n",
    "        # Removing commas and then converting to float\n",
    "        numeric_part = float(row['area'].replace(',', '').split()[0])\n",
    "        unit_part = row['area'].split()[1].lower()\n",
    "\n",
    "        if 'kanal' in unit_part:\n",
    "            return numeric_part * kanal_to_sq_meter\n",
    "        elif 'marla' in unit_part:\n",
    "            return numeric_part * marla_to_sq_meter\n",
    "        else:\n",
    "            return None  # Handle other cases if needed\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "# Applying the conversion function to the 'area' column\n",
    "isldata['area_sq_meter'] = isldata.apply(convert_to_sq_meter, axis=1)\n",
    "# Adding a new column 'price_per_sq_meter'\n",
    "isldata['price_per_sq_meter'] = isldata['price'] / isldata['area_sq_meter']\n",
    "\n",
    "# Deriving additional temporal features\n",
    "isldata['month_added'] = isldata['date_added'].dt.month\n",
    "isldata['quarter_added'] = isldata['date_added'].dt.quarter\n",
    "isldata['day_of_week_added'] = isldata['date_added'].dt.day_of_week  # Monday is 0, Sunday is 6\n",
    "\n",
    "\n",
    "#using Robust Standardization method\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "# Extracting numerical columns for scaling\n",
    "numerical_columns = ['price', 'area_sq_meter', 'price_per_sq_meter', 'baths', 'bedrooms', 'latitude', 'longitude']\n",
    "# Replacing infinite values with a large finite value\n",
    "isldata.replace([np.inf, -np.inf], np.finfo(np.float64).max, inplace=True)\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "isldata[numerical_columns] = scaler.fit_transform(isldata[numerical_columns])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'isldata' is your DataFrame\n",
    "categorical_columns = ['property_type', 'location', 'city', 'province_name', 'purpose', 'agency', 'agent']\n",
    "\n",
    "# Apply one-hot encoding\n",
    "isldata = pd.get_dummies(isldata, columns=categorical_columns)\n",
    "\n",
    "# Droping unnecessary columns\n",
    "columns_to_drop = ['property_id', 'location_id', 'page_url', 'area',  'date_added']\n",
    "isldata = isldata.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(min_samples_leaf=2, min_samples_split=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(min_samples_leaf=2, min_samples_split=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(min_samples_leaf=2, min_samples_split=10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Specifying the features (X) and target variable (y)\n",
    "features = ['area_sq_meter', 'bedrooms']\n",
    "target = 'price'\n",
    "\n",
    "X = isldata[features]\n",
    "y = isldata[target]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "#  0.2 for an 80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the model\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Defining hyperparameters to tune and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Performing Grid Search with 5-fold cross-validation(repeating process 5 times)\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "# Training the model on the entire training set\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.5925589152079705\n",
      "Mean Squared Error (MSE): 0.9004823368624048\n",
      "Root Mean Squared Error (RMSE): 0.9489374778468836\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Making predictions on the test set using the trained model\n",
    "predictions = best_rf_model.predict(X_test)\n",
    "\n",
    "# Calculating Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Calculating Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Calculating Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.77      0.80      0.79        94\n",
      "       Risky       0.17      0.15      0.16        26\n",
      "\n",
      "    accuracy                           0.66       120\n",
      "   macro avg       0.47      0.48      0.47       120\n",
      "weighted avg       0.64      0.66      0.65       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#22i-1914\n",
    "#QUESTION#2\n",
    "#Talha Kayani\n",
    "#loading data\n",
    "fraud=pd.read_csv('fraud.csv')\n",
    "\n",
    "\n",
    "# Using get_dummies to convert categorical variables into dummy variables\n",
    "fraud= pd.get_dummies(fraud, columns=['Undergrad', 'Marital.Status', 'Urban'])\n",
    "\n",
    "\n",
    "# Target Variable Transformation\n",
    "#right is equal to set false because i have not included upper limit as it can go to any range in the data\n",
    "fraud['Taxable.Income'] = pd.cut(fraud['Taxable.Income'], bins=[-float('inf'), 30000, float('inf')], labels=['Risky', 'Good'], right=False)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#deciding on what features to scale\n",
    "features_to_scale = ['Work.Experience', 'City.Population']\n",
    "#using minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "# Fit the scaler on the selected features and transform the data\n",
    "fraud[features_to_scale] = scaler.fit_transform(fraud[features_to_scale])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Features excluding the target variable\n",
    "X = fraud.drop('Taxable.Income', axis=1)  \n",
    " # Target variable\n",
    "y = fraud['Taxable.Income'] \n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "# Training the model on the training set\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Displaying the classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.77      0.85      0.81        94\n",
      "       Risky       0.12      0.08      0.10        26\n",
      "\n",
      "    accuracy                           0.68       120\n",
      "   macro avg       0.45      0.46      0.45       120\n",
      "weighted avg       0.63      0.68      0.65       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "\n",
    "# best_dt_model is trained decision tree model\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# Print classification report again after hyperparameter tuning\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
